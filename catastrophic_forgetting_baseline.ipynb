{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniyarzakarin/miniconda3/envs/clenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/daniyarzakarin/miniconda3/envs/clenv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/daniyarzakarin/miniconda3/envs/clenv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EEB3232B-F6A7-3262-948C-BB2F54905803> /Users/daniyarzakarin/miniconda3/envs/clenv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/daniyarzakarin/miniconda3/envs/clenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of task  0\n",
      "Classes in this task: [0, 1, 2, 7, 8]\n",
      "Task 0\n",
      "This task contains 30739 training examples\n",
      "This task contains 5149 test examples\n",
      "Start of task  0\n",
      "Classes in this task: [3, 4, 5, 6, 9]\n",
      "Task 0\n",
      "This task contains 29261 training examples\n",
      "This task contains 4851 test examples\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "\n",
    "benchmark = SplitMNIST(n_experiences=2)\n",
    "\n",
    "train_stream = benchmark.train_stream\n",
    "test_stream = benchmark.test_stream\n",
    "\n",
    "for experience in train_stream: \n",
    "    print(\"Start of task \", experience.task_label)\n",
    "    print('Classes in this task:', experience.classes_in_this_experience)\n",
    "\n",
    "    current_training_set = experience.dataset\n",
    "    print('Task {}'.format(experience.task_label))\n",
    "    print('This task contains', len(current_training_set), 'training examples')\n",
    "\n",
    "    current_test_set = test_stream[experience.current_experience].dataset\n",
    "    print('This task contains', len(current_test_set), 'test examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the file logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, \\\n",
    "    accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \\\n",
    "    confusion_matrix_metrics, disk_usage_metrics, StreamConfusionMatrix\n",
    "from avalanche.logging import InteractiveLogger\n",
    "\n",
    "text_logger = InteractiveLogger()\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    StreamConfusionMatrix(num_classes=benchmark.n_classes, save_image=False),\n",
    "    loggers=[text_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models and Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive, EWC\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "mlp_naive = SimpleMLP(num_classes=benchmark.n_classes)\n",
    "naive_strategy = Naive(\n",
    "    model = mlp_naive, \n",
    "    optimizer = SGD(mlp_naive.parameters(), lr=0.001, momentum=0.9),\n",
    "    criterion = CrossEntropyLoss(), \n",
    "    train_mb_size=500, \n",
    "    train_epochs=10, \n",
    "    eval_mb_size=100,\n",
    "    evaluator=eval_plugin)\n",
    "\n",
    "mlp_ewc = SimpleMLP(num_classes=benchmark.n_classes)\n",
    "ewc_strategy = EWC(\n",
    "    model = mlp_ewc, \n",
    "    optimizer = SGD(mlp_ewc.parameters(), lr=0.001, momentum=0.9),\n",
    "    criterion = CrossEntropyLoss(), \n",
    "    ewc_lambda = 2000,\n",
    "    train_mb_size=500, \n",
    "    train_epochs=10, \n",
    "    eval_mb_size=100,\n",
    "    evaluator=eval_plugin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience  0\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 62/62 [00:05<00:00, 12.19it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2072\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.4758\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6856\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9038\n",
      "100%|██████████| 62/62 [00:04<00:00, 13.11it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3935\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.3483\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9033\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8912\n",
      "100%|██████████| 62/62 [00:05<00:00, 12.12it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2881\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2399\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9202\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9163\n",
      "100%|██████████| 62/62 [00:04<00:00, 14.24it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2479\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2102\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9267\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9289\n",
      "100%|██████████| 62/62 [00:04<00:00, 14.51it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2211\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2259\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9324\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9372\n",
      "100%|██████████| 62/62 [00:05<00:00, 11.54it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2052\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9369\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9247\n",
      "100%|██████████| 62/62 [00:04<00:00, 13.18it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1940\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1626\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9397\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9498\n",
      "100%|██████████| 62/62 [00:04<00:00, 13.66it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1837\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1776\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9425\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9498\n",
      "100%|██████████| 62/62 [00:06<00:00, 10.17it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1791\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9452\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9749\n",
      "100%|██████████| 62/62 [00:04<00:00, 12.67it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1725\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1756\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9464\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9582\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the current test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 52/52 [00:01<00:00, 38.25it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1431\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9582\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[ 969,    0,    3,    0,    0,    0,    0,    1,    7,    0],\n",
      "        [   0, 1109,    7,    0,    0,    0,    0,    2,   17,    0],\n",
      "        [  15,    7,  954,    0,    0,    0,    0,   19,   37,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,   16,   30,    0,    0,    0,    0,  974,    7,    0],\n",
      "        [   9,    6,   16,    0,    0,    0,    0,   15,  928,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.1431\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.9582\n",
      "Start of experience  1\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 59/59 [00:04<00:00, 12.11it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3730\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.8170\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4516\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7739\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.26it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6459\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.4897\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8230\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8889\n",
      "100%|██████████| 59/59 [00:05<00:00,  9.97it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4675\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.4201\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8634\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8812\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.53it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3899\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.3822\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8854\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8774\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.06it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3472\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.3802\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8946\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8889\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.52it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3173\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2414\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9042\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9310\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.40it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2986\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2865\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9072\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9234\n",
      "100%|██████████| 59/59 [00:04<00:00, 12.22it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2793\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2537\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9142\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9272\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.25it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2665\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2649\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9161\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9157\n",
      "100%|██████████| 59/59 [00:04<00:00, 14.06it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2560\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2021\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9190\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9272\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the current test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 49/49 [00:01<00:00, 39.93it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.2119\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9326\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0, 953,   3,  29,   4,   0,   0,  21],\n",
      "        [  0,   0,   0,   2, 929,   2,  15,   0,   0,  34],\n",
      "        [  0,   0,   0,  38,  12, 810,  21,   0,   0,  11],\n",
      "        [  0,   0,   0,   4,  17,  24, 913,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,  19,  48,  21,   2,   0,   0, 919]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.2119\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.9326\n",
      "Final evaluation...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 52/52 [00:01<00:00, 41.03it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tExperienceForgetting/eval_phase/test_stream/Task000/Exp000 = 0.9489\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.7928\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0093\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 49/49 [00:01<00:00, 37.77it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.2119\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9326\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[  0,   0,   0,  35,   3, 879,  56,   0,   0,   7],\n",
      "        [  0,  48,   0, 990,   2,  81,  13,   0,   0,   1],\n",
      "        [  0,   0,   0, 632,  46,  31, 273,   0,   0,  50],\n",
      "        [  0,   0,   0, 953,   3,  29,   4,   0,   0,  21],\n",
      "        [  0,   0,   0,   2, 929,   2,  15,   0,   0,  34],\n",
      "        [  0,   0,   0,  38,  12, 810,  21,   0,   0,  11],\n",
      "        [  0,   0,   0,   4,  17,  24, 913,   0,   0,   0],\n",
      "        [  0,   0,   0,  72,  30,   3,   1,   0,   0, 922],\n",
      "        [  0,   0,   0, 353,  26, 425,  53,   0,   0, 117],\n",
      "        [  0,   0,   0,  19,  48,  21,   2,   0,   0, 919]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 3.0855\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.9489\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.4572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Top1_Acc_MB/train_phase/train_stream/Task000': 0.9272030651340997,\n",
       " 'Loss_MB/train_phase/train_stream/Task000': 0.20209628343582153,\n",
       " 'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9190389938826424,\n",
       " 'Loss_Epoch/train_phase/train_stream/Task000': 0.25599117917000264,\n",
       " 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.009322198485142746,\n",
       " 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 5.7928380825422785,\n",
       " 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.4572,\n",
       " 'Loss_Stream/eval_phase/test_stream/Task000': 3.085521245234832,\n",
       " 'StreamForgetting/eval_phase/test_stream': 0.9489221208001554,\n",
       " 'ConfusionMatrix_Stream/eval_phase/test_stream': tensor([[  0,   0,   0,  35,   3, 879,  56,   0,   0,   7],\n",
       "         [  0,  48,   0, 990,   2,  81,  13,   0,   0,   1],\n",
       "         [  0,   0,   0, 632,  46,  31, 273,   0,   0,  50],\n",
       "         [  0,   0,   0, 953,   3,  29,   4,   0,   0,  21],\n",
       "         [  0,   0,   0,   2, 929,   2,  15,   0,   0,  34],\n",
       "         [  0,   0,   0,  38,  12, 810,  21,   0,   0,  11],\n",
       "         [  0,   0,   0,   4,  17,  24, 913,   0,   0,   0],\n",
       "         [  0,   0,   0,  72,  30,   3,   1,   0,   0, 922],\n",
       "         [  0,   0,   0, 353,  26, 425,  53,   0,   0, 117],\n",
       "         [  0,   0,   0,  19,  48,  21,   2,   0,   0, 919]]),\n",
       " 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.9325912183055041,\n",
       " 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 0.21189222126120982,\n",
       " 'ExperienceForgetting/eval_phase/test_stream/Task000/Exp000': 0.9489221208001554}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_naive = []\n",
    "print('Starting experiment...')\n",
    "\n",
    "for exp_id, experience in enumerate(train_stream):\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "\n",
    "    naive_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the current test set')\n",
    "    results_naive.append(naive_strategy.eval(benchmark.test_stream[exp_id]))\n",
    "\n",
    "print('Final evaluation...')\n",
    "naive_strategy.eval(benchmark.test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience  0\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.46it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2564\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.5641\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6558\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8842\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.94it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4232\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.3516\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8949\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9243\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.79it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3088\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2859\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9129\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9125\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.86it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2633\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2297\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9225\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9314\n",
      "100%|██████████| 61/61 [00:04<00:00, 12.43it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2391\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9272\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9433\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.21it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2211\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1789\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9327\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9456\n",
      "100%|██████████| 61/61 [00:04<00:00, 12.66it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2105\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2462\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9358\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9220\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.54it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1979\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1981\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9398\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9291\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.36it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1904\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2167\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9385\n",
      "100%|██████████| 61/61 [00:04<00:00, 13.70it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1833\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.1649\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9436\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9551\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the current test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 52/52 [00:01<00:00, 36.97it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1416\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9575\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[ 971,    0,    3,    0,    0,    0,    0,    0,    5,    1],\n",
      "        [   0, 1114,    6,    0,    0,    0,    0,    0,   13,    2],\n",
      "        [  15,   10,  953,    0,    0,    0,    0,    0,   37,   17],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  11,    7,   17,    0,    0,    0,    0,    0,  908,   31],\n",
      "        [   9,    8,    7,    0,    0,    0,    0,    0,   19,  966]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.1416\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.9575\n",
      "Start of experience  1\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 60/60 [00:04<00:00, 12.34it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6450\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.8660\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4709\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8701\n",
      "100%|██████████| 60/60 [00:05<00:00, 10.47it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6075\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.5457\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8784\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8312\n",
      "100%|██████████| 60/60 [00:05<00:00, 10.17it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4180\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.4184\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9010\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8961\n",
      "100%|██████████| 60/60 [00:04<00:00, 12.13it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3498\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.4623\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9138\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8571\n",
      "100%|██████████| 60/60 [00:04<00:00, 12.32it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3142\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2360\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9191\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9481\n",
      "100%|██████████| 60/60 [00:04<00:00, 12.78it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2896\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.3041\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9243\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9221\n",
      "100%|██████████| 60/60 [00:05<00:00, 10.98it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2729\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2892\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9288\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8961\n",
      "100%|██████████| 60/60 [00:05<00:00, 11.50it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2566\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2481\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9336\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9481\n",
      "100%|██████████| 60/60 [00:05<00:00, 11.17it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2463\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2632\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9339\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9221\n",
      "100%|██████████| 60/60 [00:04<00:00, 12.41it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2365\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.2108\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9381\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.9221\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the current test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 49/49 [00:01<00:00, 35.71it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1650\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9509\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0, 955,   2,  23,   4,  26,   0,   0],\n",
      "        [  0,   0,   0,   1, 955,   3,  16,   7,   0,   0],\n",
      "        [  0,   0,   0,  43,  16, 811,  18,   4,   0,   0],\n",
      "        [  0,   0,   0,   4,  12,  17, 924,   1,   0,   0],\n",
      "        [  0,   0,   0,  26,  14,   0,   2, 986,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.1650\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.9509\n",
      "Final evaluation...\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 52/52 [00:01<00:00, 32.67it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tExperienceForgetting/eval_phase/test_stream/Task000/Exp000 = 0.8493\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.9334\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1082\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 49/49 [00:01<00:00, 38.22it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1650\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9509\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[  0,   0,   0,  40,   1, 836,  96,   7,   0,   0],\n",
      "        [  0, 497,   0, 468,   6,  37,  11, 116,   0,   0],\n",
      "        [  0,   0,  58, 523,  65,  32, 304,  50,   0,   0],\n",
      "        [  0,   0,   0, 955,   2,  23,   4,  26,   0,   0],\n",
      "        [  0,   0,   0,   1, 955,   3,  16,   7,   0,   0],\n",
      "        [  0,   0,   0,  43,  16, 811,  18,   4,   0,   0],\n",
      "        [  0,   0,   0,   4,  12,  17, 924,   1,   0,   0],\n",
      "        [  0,   0,   0,  26,  14,   0,   2, 986,   0,   0],\n",
      "        [  0,   0,   0, 326,  37, 532,  41,  38,   0,   0],\n",
      "        [  0,   0,   0,  25, 681,  34,   4, 265,   0,   0]])\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 2.0982\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.8493\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.5186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Top1_Acc_MB/train_phase/train_stream/Task000': 0.922077922077922,\n",
       " 'Loss_MB/train_phase/train_stream/Task000': 0.21081849932670593,\n",
       " 'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9380599790377658,\n",
       " 'Loss_Epoch/train_phase/train_stream/Task000': 0.23651596240989736,\n",
       " 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.10818713450292397,\n",
       " 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 3.9334067754578173,\n",
       " 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.5186,\n",
       " 'Loss_Stream/eval_phase/test_stream/Task000': 2.0981782685741783,\n",
       " 'StreamForgetting/eval_phase/test_stream': 0.849317738791423,\n",
       " 'ConfusionMatrix_Stream/eval_phase/test_stream': tensor([[  0,   0,   0,  40,   1, 836,  96,   7,   0,   0],\n",
       "         [  0, 497,   0, 468,   6,  37,  11, 116,   0,   0],\n",
       "         [  0,   0,  58, 523,  65,  32, 304,  50,   0,   0],\n",
       "         [  0,   0,   0, 955,   2,  23,   4,  26,   0,   0],\n",
       "         [  0,   0,   0,   1, 955,   3,  16,   7,   0,   0],\n",
       "         [  0,   0,   0,  43,  16, 811,  18,   4,   0,   0],\n",
       "         [  0,   0,   0,   4,  12,  17, 924,   1,   0,   0],\n",
       "         [  0,   0,   0,  26,  14,   0,   2, 986,   0,   0],\n",
       "         [  0,   0,   0, 326,  37, 532,  41,  38,   0,   0],\n",
       "         [  0,   0,   0,  25, 681,  34,   4, 265,   0,   0]]),\n",
       " 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.9509240246406571,\n",
       " 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 0.16497041635383572,\n",
       " 'ExperienceForgetting/eval_phase/test_stream/Task000/Exp000': 0.849317738791423}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ewc = []\n",
    "print('Starting experiment...')\n",
    "\n",
    "for exp_id, experience in enumerate(train_stream):\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "\n",
    "    ewc_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the current test set')\n",
    "    results_ewc.append(ewc_strategy.eval(benchmark.test_stream[exp_id]))\n",
    "\n",
    "print('Final evaluation...')\n",
    "ewc_strategy.eval(benchmark.test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [10/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [20/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [30/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [40/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [50/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [60/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [70/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [80/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [90/100]\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "evecs.shape : torch.Size([6, 10])\n",
      "vec_grad.shape: torch.Size([10])\n",
      "Epoch [100/100]\n"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from utilities import get_hessian_eigenvalues\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(input_size, output_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 10      # number of input features\n",
    "hidden_size = 20     # number of hidden units\n",
    "output_size = 1      # number of output units\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()           # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "inputs = torch.randn(64, input_size)   # 64 samples, each with `input_size` features\n",
    "targets = torch.randn(64, output_size) # Corresponding targets\n",
    "learning_rate = 0.01\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    evals, evecs = get_hessian_eigenvalues(model, criterion, dataset, physical_batch_size=64)\n",
    "    evecs.transpose_(1, 0)\n",
    "    # evecs = torch.transpose(evecs)\n",
    "    print(f'evecs.shape : {evecs.shape}')\n",
    "    # print(f'proj_tensor: {proj_tensor.shape}')\n",
    "\n",
    "    # Backward pass\n",
    "    # loss.backward()  # Compute gradients\n",
    "    \n",
    "    # Manually update weights\n",
    "    with torch.no_grad():  # Ensure we don’t track these operations for gradient computation\n",
    "        grad = torch.autograd.grad(loss, inputs=model.parameters(), create_graph=True)\n",
    "        vec_grad = parameters_to_vector(grad)\n",
    "        print(f'vec_grad.shape: {vec_grad.shape}')\n",
    "        step = torch.Tensor(vec_grad.shape)\n",
    "        for vec in evecs:\n",
    "            step -= learning_rate * torch.dot(vec_grad, vec) * vec  # Update each parameter by gradient descent\n",
    "        vec_params = parameters_to_vector(model.parameters())\n",
    "        vec_params += step\n",
    "        vector_to_parameters(vec_params, model.parameters())\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "\n",
    "\n",
    "# \n",
    "# loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "# for kek, lol in loader:\n",
    "#     print(kek.shape, lol.shape)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_step():\n",
    "    evals, evecs = get_hessian_eigenvalues(model, criterion, dataset, physical_batch_size=64)\n",
    "    evecs.transpose_(1, 0)\n",
    "    # evecs = torch.transpose(evecs)\n",
    "    print(f'evecs.shape : {evecs.shape}')\n",
    "    # print(f'proj_tensor: {proj_tensor.shape}')\n",
    "\n",
    "    # Backward pass\n",
    "    # loss.backward()  # Compute gradients\n",
    "    \n",
    "    # Manually update weights\n",
    "    with torch.no_grad():  # Ensure we don’t track these operations for gradient computation\n",
    "        grad = torch.autograd.grad(loss, inputs=model.parameters(), create_graph=True)\n",
    "        vec_grad = parameters_to_vector(grad)\n",
    "        print(f'vec_grad.shape: {vec_grad.shape}')\n",
    "        step = torch.Tensor(vec_grad.shape)\n",
    "        for vec in evecs:\n",
    "            step -= learning_rate * torch.dot(vec_grad, vec) * vec  # Update each parameter by gradient descent\n",
    "        vec_params = parameters_to_vector(model.parameters())\n",
    "        vec_params += step\n",
    "        vector_to_parameters(vec_params, model.parameters())\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch = 0 loss = 216.1085662841797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the image to a 784-dimensional vector\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128     # Number of units in the hidden layer\n",
    "output_size = 10      # Number of classes for MNIST digits (0–9)\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Create a subset of 10% of the training dataset\n",
    "train_size = int(0.1 * len(full_train_dataset))  # 10% of the data\n",
    "indices = np.random.choice(len(full_train_dataset), train_size, replace=False)\n",
    "train_dataset = Subset(full_train_dataset, indices)\n",
    "\n",
    "# Use the entire test dataset for evaluation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    sum_loss = torch.tensor(0.)\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        sum_loss += loss\n",
    "    print(f'On epoch = {epoch} loss = {sum_loss}')\n",
    "    evals, evecs = get_hessian_eigenvalues(model, criterion, train_dataset, physical_batch_size=64)\n",
    "    evecs.transpose_(1, 0)\n",
    "    # Manually update weights\n",
    "    with torch.no_grad():  # Ensure we don’t track these operations for gradient computation\n",
    "        grad = torch.autograd.grad(loss, inputs=model.parameters(), create_graph=True)\n",
    "        vec_grad = parameters_to_vector(grad)\n",
    "        print(f'vec_grad.shape: {vec_grad.shape}')\n",
    "        step = torch.Tensor(vec_grad.shape)\n",
    "        for vec in evecs:\n",
    "            step -= learning_rate * torch.dot(vec_grad, vec) * vec  # Update each parameter by gradient descent\n",
    "        vec_params = parameters_to_vector(model.parameters())\n",
    "        vec_params += step\n",
    "        vector_to_parameters(vec_params, model.parameters())\n",
    "        model.zero_grad()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([241, 6])\n"
     ]
    }
   ],
   "source": [
    "print(evecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
